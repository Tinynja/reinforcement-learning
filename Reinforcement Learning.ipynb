{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122a63ce",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: An Introduction\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeab862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables used for kernel and git management\n",
    "import ipynbname\n",
    "NOTEBOOK_NAME = ipynbname.name()\n",
    "\n",
    "KERNEL_VENV = '.venv'\n",
    "KERNEL_NAME = 'rl'\n",
    "KERNEL_DISPLAY_NAME = 'RL-venv'\n",
    "\n",
    "KERNEL_SAVED = !grep -A4 \"kernelspec\" \"{NOTEBOOK_NAME}.ipynb\" | grep -Po \"(?<=\\\"display_name\\\": \\\")[^,\\\"]+\"\n",
    "KERNEL_SAVED = KERNEL_SAVED[0]\n",
    "                                                                      \n",
    "IS_INTERACTIVE = ![ ! -e \"{NOTEBOOK_NAME}.lock\" ] && echo 1\n",
    "IS_INTERACTIVE = IS_INTERACTIVE[0] if IS_INTERACTIVE else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$KERNEL_VENV\" \"$KERNEL_NAME\" \"$KERNEL_DISPLAY_NAME\" \"$KERNEL_SAVED\"\n",
    "# Create a venv and a kernel for the notebook\n",
    "KERNEL_VENV=$1\n",
    "KERNEL_NAME=$2\n",
    "KERNEL_DISPLAY_NAME=$3\n",
    "KERNEL_SAVED=$4\n",
    "\n",
    "if [ \"$(. \"$KERNEL_VENV/bin/activate\" 2> /dev/null && which python)\" != \"$(pwd)/$KERNEL_VENV/bin/python\" ]; then\n",
    "    python -m venv \"$KERNEL_VENV\" --prompt \"$KERNEL_DISPLAY_NAME\" --system-site-packages;\n",
    "    echo \"Created virtual environment: '$(pwd)/$KERNEL_VENV'\"\n",
    "    OUTPUT_NOT_EMPTY=1\n",
    "fi\n",
    "\n",
    "if ! jupyter kernelspec list | grep -q $KERNEL_NAME; then\n",
    "    (. \"$KERNEL_VENV/bin/activate\"; python -m ipykernel install --user --name=$KERNEL_NAME --display-name=\"$KERNEL_DISPLAY_NAME\")\n",
    "    echo \"## Please refresh page and select: Kernel > Change kernel > $KERNEL_DISPLAY_NAME ##\"\n",
    "    OUTPUT_NOT_EMPTY=1\n",
    "elif [ \"$KERNEL_SAVED\" != \"$KERNEL_DISPLAY_NAME\" ]; then\n",
    "    echo \"## Saved kernel is '$KERNEL_SAVED'. Please refresh page and select: Kernel > Change kernel > $KERNEL_DISPLAY_NAME ##\"\n",
    "    OUTPUT_NOT_EMPTY=1\n",
    "fi\n",
    "\n",
    "if [ -z \"$OUTPUT_NOT_EMPTY\" ]; then echo \"No news is good news!\"; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_INTERACTIVE:\n",
    "    COMMIT_PRESAVE = ['false','true'][input(\"Did you save the notebook? [y/N] \").lower() in ['y','yes']]\n",
    "    COMMIT_MESSAGE = input(\"Next commit message: \")\n",
    "    COMMIT_AMEND = ['','--amend'][input(\"Amend next commit? [y/N] \").lower() in ['y','yes']]\n",
    "else:\n",
    "    COMMIT_PRESAVE = 'false'\n",
    "    COMMIT_MESSAGE = ''\n",
    "    COMMIT_AMEND = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$IS_INTERACTIVE\" \"$NOTEBOOK_NAME\" \"$KERNEL_SAVED\" \"$KERNEL_DISPLAY_NAME\" \"$COMMIT_PRESAVE\" \"$COMMIT_MESSAGE\" \"$COMMIT_AMEND\"\n",
    "# Commit and push to git\n",
    "IS_INTERACTIVE=$1\n",
    "NOTEBOOK_NAME=$2\n",
    "KERNEL_SAVED=$3\n",
    "KERNEL_DISPLAY_NAME=$4\n",
    "COMMIT_PRESAVE=$5\n",
    "COMMIT_MESSAGE=$6\n",
    "COMMIT_AMEND=$7\n",
    "\n",
    "ALL_CHECKS_PASSED=false\n",
    "if [ -z $IS_INTERACTIVE ]; then\n",
    "    echo \"INFO: Skipping git section.\"\n",
    "elif [ \"$IS_INTERACTIVE\" = \"\\$IS_INTERACTIVE\" ]; then\n",
    "    echo \"ERROR: Missing variables. Please execute all cells above.\"\n",
    "elif ! $COMMIT_PRESAVE; then\n",
    "    echo \"ERROR: Notebook has not been saved to disk. Please save and execute the cell above.\"\n",
    "elif [ -z \"$COMMIT_MESSAGE\" ]; then\n",
    "    echo \"ERROR: Commit message cannot be empty. Please execute the cell above.\"\n",
    "elif [ \"$KERNEL_SAVED\" != \"$KERNEL_DISPLAY_NAME\" ]; then\n",
    "    echo \"ERROR: Wrong kernel saved. Please select the '$KERNEL_DISPLAY_NAME' kernel and save the notebook.\"\n",
    "elif ! git checkout develop 1> /dev/null; then\n",
    "    echo \"ERROR: Couldn't checkout develop. Manual intervention required.\"\n",
    "else\n",
    "    ALL_CHECKS_PASSED=true\n",
    "fi\n",
    "\n",
    "if $ALL_CHECKS_PASSED; then\n",
    "    # Backup and prepare\n",
    "    mkdir -p .backups\n",
    "    cp \"$NOTEBOOK_NAME\".ipynb .backups/\n",
    "    mv \"$NOTEBOOK_NAME\".ipynb \"$NOTEBOOK_NAME\".lock\n",
    "    # Run and commit to develop\n",
    "    python -m nbconvert --clear-output \"$NOTEBOOK_NAME\".lock\n",
    "    git add \"$NOTEBOOK_NAME\".ipynb\n",
    "    git commit $COMMIT_AMEND -m \"$COMMIT_MESSAGE\"\n",
    "    git push $([ -n $COMMIT_AMEND ] && echo -f)\n",
    "    cp \".backups/$NOTEBOOK_NAME\".ipynb .\n",
    "    rm \"$NOTEBOOK_NAME\".lock\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "from itertools import product\n",
    "\n",
    "# Pypi libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib configs\n",
    "# %config InlineBackend.figure_formats = ['svg']\n",
    "# plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faca5f2",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "\n",
    "### 2.3 - The 10-armed Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd922979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmedBandit:\n",
    "    def __init__(self,arms,runs):\n",
    "        self.arms = arms\n",
    "        self.runs = runs\n",
    "        self.runs_range = np.arange(self.runs)\n",
    "        \n",
    "        self.action_values = np.random.normal(0,1,size=(self.arms,self.runs))\n",
    "        self.optimal_action = self.action_values.argmax(axis=0)\n",
    "    \n",
    "    def step(self,action):\n",
    "        return np.random.normal(self.action_values[action,self.runs_range],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3a647",
   "metadata": {},
   "source": [
    "To create a policy, one must create a class that inherits from 3 base classes:\n",
    "  - The **BasePolicy** class\n",
    "  - An _ActionSelection_ class\n",
    "  - An _UpdateRule_ class\n",
    "  \n",
    "For example, to create an epsilon greedy policy with sample averaging:\n",
    "\n",
    "    class EpsilonSampleAveragePolicy(BasePolicy,EpsilonActionSelection,SampleAverageUpdateRule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    def __init__(self,actions,runs,initial_values=0):\n",
    "        self.actions = actions\n",
    "        self.runs = runs\n",
    "        self.runs_arange = np.arange(runs)\n",
    "        \n",
    "        self.action_values = np.ones((actions,runs))*initial_values\n",
    "        self.action_count = np.zeros((actions,runs))\n",
    "        self.prev_action = np.zeros(runs,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyActionSelection:    \n",
    "    def act(self):\n",
    "        self.prev_action = self.action_values.argmax(axis=0)\n",
    "        self.action_count[self.prev_action,self.runs_arange] += 1\n",
    "        return self.prev_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a26d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonActionSelection:\n",
    "    def __init__(self,epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def act(self):\n",
    "        # action_type: 0=random, 1=greedy\n",
    "        action_type = np.random.rand(self.runs) > self.epsilon\n",
    "        \n",
    "        # random actions\n",
    "        self.prev_action[~action_type] = np.random.randint(self.actions,size=sum(~action_type))\n",
    "        # greedy actions\n",
    "        self.prev_action[action_type] = self.action_values[:,action_type].argmax(axis=0)\n",
    "        \n",
    "        self.action_count[self.prev_action,self.runs_arange] += 1\n",
    "        return self.prev_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleAverageUpdateRule:\n",
    "    def update(self,reward):\n",
    "        # Q_n+1 = Q_n + 1/n * (R_n - Q_n)\n",
    "        self.action_values[self.prev_action,self.runs_arange] += 1/self.action_count[self.prev_action,self.runs_arange] * \\\n",
    "                                                                 (reward - self.action_values[self.prev_action,self.runs_arange])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySampleAveragePolicy(BasePolicy,GreedyActionSelection,SampleAverageUpdateRule):\n",
    "    def __init__(self,actions,runs,initial_values=0):\n",
    "        BasePolicy.__init__(self,actions,runs,initial_values=initial_values)\n",
    "        GreedyActionSelection.__init__(self)\n",
    "        SampleAverageUpdateRule.__init__(self)\n",
    "\n",
    "class EpsilonSampleAveragePolicy(BasePolicy,EpsilonActionSelection,SampleAverageUpdateRule):\n",
    "    def __init__(self,actions,runs,epsilon,initial_values=0):\n",
    "        BasePolicy.__init__(self,actions,runs,initial_values=initial_values)\n",
    "        EpsilonActionSelection.__init__(self,epsilon)\n",
    "        SampleAverageUpdateRule.__init__(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddfd14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAnalyser:        \n",
    "    def __init__(self,episode_length):\n",
    "        self.episode_length = episode_length\n",
    "        self.labels = []\n",
    "        self.reward_history = np.zeros((0,episode_length))\n",
    "        self.optimal_history = np.zeros((0,episode_length))\n",
    "        \n",
    "    def create(self,label,env,policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        \n",
    "        self.step = 0\n",
    "        self.labels.append(label)\n",
    "        self.reward_history = np.concatenate((self.reward_history,np.zeros((1,self.episode_length))))\n",
    "        self.optimal_history = np.concatenate((self.optimal_history,np.zeros((1,self.episode_length))))\n",
    "        \n",
    "    def log(self,reward):\n",
    "        self.reward_history[-1,self.step] = reward.mean()\n",
    "        self.optimal_history[-1,self.step] = (self.policy.prev_action == self.env.optimal_action).sum()/self.env.runs\n",
    "        self.step += 1\n",
    "    \n",
    "    def plot_reward_history(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.reward_history.transpose(),label=self.labels,linewidth=0.75)\n",
    "        plt.legend()\n",
    "    \n",
    "    def plot_optimal_history(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.optimal_history.transpose(),label=self.labels,linewidth=1)\n",
    "        plt.legend()\n",
    "        \n",
    "def run_bandit_episode(env,policy,datalog,episode_length):\n",
    "    for i in range(episode_length):\n",
    "        action = policy.act()\n",
    "        reward = env.step(action)\n",
    "        policy.update(reward)\n",
    "        datalog.log(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eeae0d",
   "metadata": {},
   "source": [
    "#### $\\epsilon$-greedy policy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "arms = 10\n",
    "episode_length = 1000\n",
    "runs = 2000\n",
    "epsilons = (0,0.01,0.1)\n",
    "\n",
    "datalog = PolicyAnalyser(episode_length)\n",
    "for eps in epsilons:\n",
    "    env = ArmedBandit(arms,runs)\n",
    "    if eps == 0:\n",
    "        policy = GreedySampleAveragePolicy(arms,runs)\n",
    "    else:\n",
    "        policy = EpsilonSampleAveragePolicy(arms,runs,eps)\n",
    "    datalog.create(f'eps={eps}',env,policy)\n",
    "    run_bandit_episode(env,policy,datalog,episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adc8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalog.plot_reward_history()\n",
    "datalog.plot_optimal_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd9035",
   "metadata": {},
   "source": [
    "### 2.5 - Tracking a Nonstationary Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmedBanditNonStationary(ArmedBandit):\n",
    "    # Armed Bandit problem with action values that change randomly every step\n",
    "    def __init__(self,arms,runs,random_walk_std_dev):\n",
    "        super().__init__(arms,runs)\n",
    "        self._step = super().step\n",
    "        self.action_values = np.zeros((self.arms,self.runs))\n",
    "        self.random_walk_std_dev = random_walk_std_dev\n",
    "        \n",
    "    def step(self,*args,**kwargs):\n",
    "        self.action_values = np.random.normal(self.action_values,self.random_walk_std_dev)\n",
    "        self.optimal_action = self.action_values.argmax(axis=0)\n",
    "        return self._step(*args,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d961802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantStepUpdateRule:\n",
    "    def __init__(self,alpha):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def update(self,reward):\n",
    "        # Q_n+1 = Q_n + alpha * (R_n - Q_n)\n",
    "        self.action_values[self.prev_action,self.runs_arange] += self.alpha * (reward - self.action_values[self.prev_action,self.runs_arange])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonConstantStepPolicy(BasePolicy,EpsilonActionSelection,ConstantStepUpdateRule):\n",
    "    def __init__(self,actions,runs,epsilon,alpha,initial_values=0):\n",
    "        BasePolicy.__init__(self,actions,runs)\n",
    "        EpsilonActionSelection.__init__(self,epsilon)\n",
    "        ConstantStepUpdateRule.__init__(self,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc12cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arms = 10\n",
    "episode_length = 5000\n",
    "runs = 2000\n",
    "epsilons = (0.01,0.1,)\n",
    "alphas = (0.05,0.1,0.2)\n",
    "random_walk_std_dev = 0.01\n",
    "\n",
    "datalog = PolicyAnalyser(episode_length)\n",
    "for eps,alpha in product(epsilons,alphas):\n",
    "    env = ArmedBanditNonStationary(arms,runs,random_walk_std_dev)\n",
    "    policy = EpsilonConstantStepPolicy(arms,runs,eps,alpha)\n",
    "    datalog.create(f'eps={eps},alpha={alpha}',env,policy)\n",
    "    run_bandit_episode(env,policy,datalog,episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalog.plot_reward_history()\n",
    "datalog.plot_optimal_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df701b",
   "metadata": {},
   "source": [
    "#### Exercice 2.5 - Nonstationary Bandit Problem with sample-averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "arms = 10\n",
    "episode_length = 10000\n",
    "runs = 2000\n",
    "epsilons = (0,0.01,0.1)\n",
    "random_walk_std_dev = 0.01\n",
    "\n",
    "datalog = PolicyAnalyser(episode_length)\n",
    "for eps in epsilons:\n",
    "    env = ArmedBanditNonStationary(arms,runs,random_walk_std_dev)\n",
    "    policy = EpsilonSampleAveragePolicy(arms,runs,eps)\n",
    "    datalog.create(f'eps={eps}',env,policy)\n",
    "    run_bandit_episode(env,policy,datalog,episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalog.plot_reward_history()\n",
    "datalog.plot_optimal_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc68cf",
   "metadata": {},
   "source": [
    "### 2.6 - Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40025c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-venv",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
