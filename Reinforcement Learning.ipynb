{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122a63ce",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: An Introduction\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeab862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables used for kernel and git management\n",
    "import ipynbname\n",
    "NOTEBOOK_NAME = ipynbname.name()\n",
    "\n",
    "KERNEL_VENV = '.venv'\n",
    "KERNEL_NAME = 'rl'\n",
    "KERNEL_DISPLAY_NAME = 'RL-venv'\n",
    "\n",
    "KERNEL_SAVED = !grep -A4 \"kernelspec\" \"{NOTEBOOK_NAME}.ipynb\" | grep -Po \"(?<=\\\"display_name\\\": \\\")[^,\\\"]+\"\n",
    "KERNEL_SAVED = KERNEL_SAVED[0]\n",
    "                                                                      \n",
    "IS_INTERACTIVE = ![ ! -e \"{NOTEBOOK_NAME}.lock\" ] && echo 1\n",
    "IS_INTERACTIVE = IS_INTERACTIVE[0] if IS_INTERACTIVE else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$KERNEL_VENV\" \"$KERNEL_NAME\" \"$KERNEL_DISPLAY_NAME\" \"$KERNEL_SAVED\"\n",
    "# Create a venv and a kernel for the notebook\n",
    "KERNEL_VENV=$1\n",
    "KERNEL_NAME=$2\n",
    "KERNEL_DISPLAY_NAME=$3\n",
    "KERNEL_SAVED=$4\n",
    "\n",
    "if [ \"$(. \"$KERNEL_VENV/bin/activate\" 2> /dev/null && which python)\" != \"$(pwd)/$KERNEL_VENV/bin/python\" ]; then\n",
    "    python -m venv \"$KERNEL_VENV\" --prompt \"$KERNEL_DISPLAY_NAME\" --system-site-packages;\n",
    "    echo \"Created virtual environment: '$(pwd)/$KERNEL_VENV'\"\n",
    "    OUTPUT_NOT_EMPTY=1\n",
    "fi\n",
    "\n",
    "if ! jupyter kernelspec list | grep -q $KERNEL_NAME; then\n",
    "    (. \"$KERNEL_VENV/bin/activate\"; python -m ipykernel install --user --name=$KERNEL_NAME --display-name=\"$KERNEL_DISPLAY_NAME\")\n",
    "    echo \"## Please refresh page and select: Kernel > Change kernel > $KERNEL_DISPLAY_NAME ##\"\n",
    "    OUTPUT_NOT_EMPTY=1\n",
    "elif [ \"$KERNEL_SAVED\" != \"$KERNEL_DISPLAY_NAME\" ]; then\n",
    "    echo \"## Saved kernel is '$KERNEL_SAVED'. Please refresh page and select: Kernel > Change kernel > $KERNEL_DISPLAY_NAME ##\"\n",
    "    OUTPUT_NOT_EMPTY=1\n",
    "fi\n",
    "\n",
    "if [ -z \"$OUTPUT_NOT_EMPTY\" ]; then echo \"No news is good news!\"; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_INTERACTIVE:\n",
    "    COMMIT_MESSAGE = input(\"Next commit message: \")\n",
    "    COMMIT_AMEND = ['','--amend'][input(\"Amend next commit? [y/N] \").lower() in ['y','yes']]\n",
    "else:\n",
    "    COMMIT_MESSAGE = ''\n",
    "    COMMIT_AMEND = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$IS_INTERACTIVE\" \"$NOTEBOOK_NAME\" \"$KERNEL_SAVED\" \"$KERNEL_DISPLAY_NAME\" \"$COMMIT_MESSAGE\" \"$COMMIT_AMEND\"\n",
    "# Commit and push to git\n",
    "IS_INTERACTIVE=$1\n",
    "NOTEBOOK_NAME=$2\n",
    "KERNEL_SAVED=$3\n",
    "KERNEL_DISPLAY_NAME=$4\n",
    "COMMIT_MESSAGE=$5\n",
    "COMMIT_AMEND=$6\n",
    "\n",
    "ALL_CHECKS_PASSED=false\n",
    "if [ -z $IS_INTERACTIVE ]; then\n",
    "    echo \"INFO: Skipping git section.\"\n",
    "elif [ -z \"$COMMIT_MESSAGE\" ]; then\n",
    "    echo \"ERROR: Commit message cannot be empty. Please execute the cell above.\"\n",
    "elif [ \"$COMMIT_MESSAGE\" = \"\\$COMMIT_MESSAGE\" ]; then\n",
    "    echo \"ERROR: Missing variables. Please execute all cells above.\"\n",
    "elif [ \"$KERNEL_SAVED\" != \"$KERNEL_DISPLAY_NAME\" ]; then\n",
    "    echo \"ERROR: Wrong kernel saved. Please select the '$KERNEL_DISPLAY_NAME' kernel and save the notebook.\"\n",
    "elif ! git checkout develop 1> /dev/null; then\n",
    "    echo \"ERROR: Couldn't checkout develop. Manual intervention required.\"\n",
    "else\n",
    "    ALL_CHECKS_PASSED=true\n",
    "fi\n",
    "\n",
    "if $ALL_CHECKS_PASSED; then\n",
    "    mkdir -p .backups\n",
    "    cp \"$NOTEBOOK_NAME\".ipynb .backups/\n",
    "    mv \"$NOTEBOOK_NAME\".ipynb \"$NOTEBOOK_NAME\".lock\n",
    "    python -m nbconvert --clear-output \"$NOTEBOOK_NAME\".lock\n",
    "    git add \"$NOTEBOOK_NAME\".ipynb\n",
    "    git commit $COMMIT_AMEND -m \"$COMMIT_MESSAGE\"\n",
    "#     git push $([ -n $COMMIT_AMEND ] && echo -f)\n",
    "    cp \".backups/$NOTEBOOK_NAME\".ipynb .\n",
    "    rm \"$NOTEBOOK_NAME\".lock\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pypi libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib configs\n",
    "# %config InlineBackend.figure_formats = ['svg']\n",
    "# plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faca5f2",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "\n",
    "### 2.3 - The 10-armed Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd922979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmedBandit:\n",
    "    def __init__(self,arms,runs):\n",
    "        self.arms = arms\n",
    "        self.runs = runs\n",
    "        self.runs_range = np.arange(self.runs)\n",
    "        \n",
    "        self.action_values = np.random.normal(0,1,size=(self.arms,self.runs))\n",
    "        self.optimal_action = self.action_values.argmax(axis=0)\n",
    "    \n",
    "    def step(self,action):\n",
    "        return np.random.normal(self.action_values[action,self.runs_range],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71743a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonPolicy:\n",
    "    def __init__(self,actions,runs,epsilon):\n",
    "        self.actions = actions\n",
    "        self.runs = runs\n",
    "        self.runs_arange = np.arange(runs)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.action_reward = np.zeros((actions,runs))\n",
    "        self.action_count = np.zeros((actions,runs))\n",
    "        self.prev_action = np.zeros(runs,dtype=int)\n",
    "    \n",
    "    def act(self):\n",
    "        # action_type: 0=random, 1=greedy\n",
    "        action_type = np.random.rand(self.runs) > self.epsilon\n",
    "        \n",
    "        # random actions\n",
    "        self.prev_action[~action_type] = np.random.randint(self.actions,size=sum(~action_type))\n",
    "        # greedy actions: argmax estimate action values via sample average (replace 0 counts by 1 to avoid div0 error)\n",
    "        self.prev_action[action_type] = (self.action_reward[:,action_type] / \\\n",
    "                                         (self.action_count[:,action_type] + (self.action_count[:,action_type] == 0)) \\\n",
    "                                        ).argmax(axis=0)\n",
    "        return self.prev_action\n",
    "    \n",
    "    def update(self,reward):\n",
    "        self.action_reward[self.prev_action,self.runs_arange] += reward\n",
    "        self.action_count[self.prev_action,self.runs_arange] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7817b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAnalyser:        \n",
    "    def __init__(self,episode_length):\n",
    "        self.episode_length = episode_length\n",
    "        self.labels = []\n",
    "        self.reward_history = np.zeros((0,episode_length))\n",
    "        self.optimal_history = np.zeros((0,episode_length))\n",
    "        \n",
    "    def create(self,label,env,policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        \n",
    "        self.step = 0\n",
    "        self.labels.append(label)\n",
    "        self.reward_history = np.concatenate((self.reward_history,np.zeros((1,self.episode_length))))\n",
    "        self.optimal_history = np.concatenate((self.optimal_history,np.zeros((1,self.episode_length))))\n",
    "        \n",
    "    def log(self,reward):\n",
    "        self.reward_history[-1,self.step] = reward.mean()\n",
    "        self.optimal_history[-1,self.step] = (self.policy.prev_action == self.env.optimal_action).sum()/self.env.runs\n",
    "        self.step += 1\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.reward_history.transpose(),label=self.labels,linewidth=0.75)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(self.optimal_history.transpose(),label=self.labels,linewidth=1)\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bandit_episode(env,policy,datalog,episode_length):\n",
    "    for i in range(episode_length):\n",
    "        action = policy.act()\n",
    "        reward = env.step(action)\n",
    "        policy.update(reward)\n",
    "        datalog.log(reward)\n",
    "\n",
    "arms = 10\n",
    "episode_length = 1000\n",
    "runs = 2000\n",
    "epsilon = [0,0.01,0.1]\n",
    "\n",
    "datalog = PolicyAnalyser(episode_length)\n",
    "for eps in epsilon:\n",
    "    env = ArmedBandit(arms,runs)\n",
    "    policy = EpsilonPolicy(arms,runs,eps)\n",
    "    datalog.create(f'eps={eps}',env,policy)\n",
    "    run_bandit_episode(env,policy,datalog,episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalog.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8cbd3",
   "metadata": {},
   "source": [
    "## Exercice 2.5 - Nonstationary Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-stationnary Armed Bandit\n",
    "class NSArmedBandit(ArmedBandit):\n",
    "    # Armed Bandit problem with action values initialised to 0\n",
    "    # and a random walk every step\n",
    "    def __init__(self,action_value_dev,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self._step = super().step\n",
    "        self.action_values = np.zeros((self.arms,self.runs))\n",
    "        self.action_value_dev = action_value_dev\n",
    "        \n",
    "    def step(self,*args,**kwargs):\n",
    "        self.action_values = np.random.normal(self.action_values,self.action_value_dev)\n",
    "        self.optimal_action = self.action_values.argmax(axis=0)\n",
    "        return self._step(*args,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_walk_std_dev = 0.01\n",
    "\n",
    "datalog = PolicyAnalyser(episode_length)\n",
    "for eps in epsilon:\n",
    "    env = NSArmedBandit(random_walk_std_dev,arms,runs)\n",
    "    policy = EpsilonPolicy(arms,runs,eps)\n",
    "    datalog.create(f'eps={eps}',env,policy)\n",
    "    run_bandit_episode(env,policy,datalog,episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalog.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-venv",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
